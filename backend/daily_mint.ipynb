{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9db8e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63ee0dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_helpers import *\n",
    "from web3 import Web3\n",
    "import os, json\n",
    "from supabase import create_client, Client\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d17d69",
   "metadata": {},
   "source": [
    "# Nightly Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f7489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    all_data = {\n",
    "        \"categories\": pd.DataFrame(get_data(\"categories\", \"*\")),\n",
    "        \"comments\": pd.DataFrame(get_data(\"comments\", \"*\")),\n",
    "        \"follows\": pd.DataFrame(get_data(\"follows\", \"*\")),\n",
    "        \"posts\": pd.DataFrame(get_data(\"posts\", \"*\")),\n",
    "        \"users\": pd.DataFrame(get_data(\"users\", \"*\")),\n",
    "        \"votes\": pd.DataFrame(get_data(\"votes\", \"*\")),\n",
    "    }\n",
    "    \n",
    "    # Daily payout\n",
    "    for idx, row in all_data['users'][all_data['users']['daily_payout_claimed'] == True].iterrows():\n",
    "        updates = {\n",
    "            \"daily_payout_claimed\": False, \n",
    "            \"tokens_to_claim\": row['tokens_to_claim'] + row['daily_payout_value']\n",
    "        }\n",
    "        supabase.table(\"users\").update(updates).eq(\"id\", row['id']).execute()\n",
    "\n",
    "    all_users = all_data['users'].copy()\n",
    "    all_users.loc[all_users['username'].isna(), :] = update_scores(all_data, all_users[all_users['username'].isna()])\n",
    "    all_users.loc[~all_users['username'].isna(), :] = update_scores(all_data, all_users[~all_users['username'].isna()])\n",
    "\n",
    "    for idx, row in all_users.iterrows():\n",
    "        updates = {\n",
    "            'social_score': row['social_score'],\n",
    "            'level': row['level'],\n",
    "            'exp': row['exp'],\n",
    "            'exp_to_next_level': row['exp_to_next_level'],\n",
    "            'daily_payout_value': row['daily_payout_value'],\n",
    "        }\n",
    "        supabase.table(\"users\").update(updates).eq(\"id\", row['id']).execute()\n",
    "    \n",
    "    time.sleep(60*60*24)\n",
    "    \n",
    "    # Reddit Airdrop\n",
    "\n",
    "    # Update airdrop value\n",
    "    for idx, row in all_data['users'][~all_data['users']['reddit_username'].isna() & all_data['users']['reddit_airdrop_value'].isna()  & ~all_data['users']['username'].isna()].iterrows():\n",
    "        user = reddit.redditor(row['reddit_username'])   \n",
    "        try:\n",
    "            airdrop_value = user.total_karma\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        supabase.table(\"users\").update({\"reddit_airdrop_value\": airdrop_value}).eq(\"id\", row['id']).execute()\n",
    "    print(\"Done update airdrop value\")\n",
    "\n",
    "    # Check for claims\n",
    "    for idx, row in all_data['users'][all_data['users']['reddit_airdrop_claimed'] == \"pending\"].iterrows():\n",
    "        if row['reddit_id'] is None:\n",
    "            supabase.table(\"users\").update({\"reddit_airdrop_claimed\": \"rejected\"}).eq(\"id\", row['id']).execute()\n",
    "\n",
    "        user = reddit.redditor(reddit_username)  \n",
    "        try:\n",
    "            for i, post in enumerate(user.new()):\n",
    "                if i > 10:\n",
    "                    break\n",
    "                if type(post) == praw.models.reddit.submission.Submission:\n",
    "                    if row[\"wallet_address_personal\"] in post.selftext:\n",
    "                        airdrop_value = user.total_karma\n",
    "                        updates = {\n",
    "                            \"reddit_airdrop_value\": airdrop_value,\n",
    "                            \"reddit_airdrop_claimed\": \"claimed\",\n",
    "                            \"tokens_to_claim\": row['tokens_to_claim'] + airdrop_value,\n",
    "                        }\n",
    "                        supabase.table(\"users\").update(updates).eq(\"id\", row['id']).execute()\n",
    "        except:\n",
    "            supabase.table(\"users\").update({\"reddit_airdrop_claimed\": \"rejected\"}).eq(\"id\", row['id']).execute()\n",
    "            \n",
    "    ## Blockchain stuff\n",
    "    \n",
    "    # Transfer tokens\n",
    "\n",
    "    for idx, row in all_data['users'][(all_data['users']['tokens_claimed'] == True) & (all_data['users']['tokens_to_claim'] != 0) & (~all_data['users']['wallet_address_personal'].isna())].iterrows():\n",
    "        with open(\"new_announcements.txt\", \"a\") as file_object:\n",
    "            file_object.write(json.dumps({\"type\": \"mint_tokens\", \"wallet\": row[\"wallet_address_personal\"], \"amount\": row[\"tokens_to_claim\"]}) + '\\n')\n",
    "        supabase.table(\"users\").update({\"tokens_claimed\": False, \"tokens_to_claim\": 0}).eq(\"id\", row['id']).execute()\n",
    "        \n",
    "    # Mint users\n",
    "    top_users = all_data['users'][~all_data['users']['username'].isna() & all_data['users']['msa_id'].isna()]\n",
    "    top_users = top_users.sort_values(\"exp\")\n",
    "\n",
    "    for idx, row in top_users.iterrows():\n",
    "        print(row['username'])\n",
    "        wallet = get_wallet_from_username(row['username'])\n",
    "        with open(\"new_announcements.txt\", \"a\") as file_object:\n",
    "            file_object.write(json.dumps({\"type\": \"new_user\", \"private_key\": wallet.privateKey.hex(), \"row_id\": row['id']}) + '\\n')\n",
    "        supabase.table(\"users\").update({\"wallet_address_provided\": wallet.address}).eq(\"id\", row['id']).execute()\n",
    "\n",
    "    # Mint users with msa\n",
    "    top_users = all_data['users'][~all_data['users']['username'].isna() & ~all_data['users']['msa_id'].isna()]\n",
    "    top_users = top_users.sort_values(\"exp\")\n",
    "\n",
    "    for idx, row in top_users.iterrows():\n",
    "        wallet = get_wallet_from_username(row['username'])\n",
    "        message = mint_user(row['msa_id'], row['profile_pic'], wallet.address)\n",
    "        with open(\"new_announcements.txt\", \"a\") as file_object:\n",
    "            file_object.write(json.dumps({\"type\": \"mint_data\", \"message\": message, \"schemaId\": schemas[\"user\"], \"user_msa_id\": row[\"msa_id\"], \"row_id\": row['id']}) + '\\n')\n",
    "            \n",
    "    # Mint posts\n",
    "    top_posts = all_data['posts'][all_data['posts']['reddit_id'].isna() & all_data['posts']['transaction_hash'].isna() & ~all_data['posts']['ipfs_hash'].isna()]\n",
    "    top_posts = top_posts.join(all_data['users'].set_index('id')[['username', 'msa_id', 'tokens_to_claim']], on=\"user_id\")\n",
    "    top_posts = top_posts[~top_posts['msa_id'].isna() & ~top_posts['username'].isna() & top_posts['tokens_to_claim'] > 5]\n",
    "    top_posts = top_posts.join(all_data['votes'].groupby(\"post_id\").sum()[['up']], on=\"id\", how=\"inner\")\n",
    "    top_posts = top_posts.sort_values(\"up\")\n",
    "\n",
    "    for idx, row in top_posts.iterrows():\n",
    "        message = json.dumps({\"ipfs_hash\": row['ipfs_hash']})\n",
    "        with open(\"new_announcements.txt\", \"a\") as file_object:\n",
    "            file_object.write(json.dumps({\"type\": \"mint_data\", \"message\": message, \"schemaId\": \"post\", \"user_msa_id\": row[\"msa_id\"], \"row_id\": row['id']}) + '\\n')\n",
    "        supabase.table(\"users\").update({\"tokens_claimed\": False, \"tokens_to_claim\": 0}).eq(\"id\", row['id']).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73697af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7d5c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5becdc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e030a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {\n",
    "    \"categories\": pd.DataFrame(get_data(\"categories\", \"*\")),\n",
    "    \"comments\": pd.DataFrame(get_data(\"comments\", \"*\")),\n",
    "    \"follows\": pd.DataFrame(get_data(\"follows\", \"*\")),\n",
    "    \"posts\": pd.DataFrame(get_data(\"posts\", \"*\")),\n",
    "    \"users\": pd.DataFrame(get_data(\"users\", \"*\")),\n",
    "    \"votes\": pd.DataFrame(get_data(\"votes\", \"*\")),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b28101d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\web3_messing_around\\PostThread-Polygon\\backend\\db_helpers.py:154: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['social_score'] = social_score\n",
      "F:\\web3_messing_around\\PostThread-Polygon\\backend\\db_helpers.py:155: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['level'] = level\n",
      "F:\\web3_messing_around\\PostThread-Polygon\\backend\\db_helpers.py:156: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['overall_score'] = overall_score\n",
      "F:\\web3_messing_around\\PostThread-Polygon\\backend\\db_helpers.py:157: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['overall_score'] /= max(overall_score)\n",
      "F:\\web3_messing_around\\PostThread-Polygon\\backend\\db_helpers.py:158: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['exp'] = exp\n",
      "F:\\web3_messing_around\\PostThread-Polygon\\backend\\db_helpers.py:159: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['exp_to_next_level'] = exp_to_next_level\n",
      "F:\\web3_messing_around\\PostThread-Polygon\\backend\\db_helpers.py:160: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['daily_payout_value'] = daily_token_rewards * df['overall_score']\n",
      "F:\\web3_messing_around\\PostThread-Polygon\\backend\\db_helpers.py:161: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[df['daily_payout_value'] < 100] = 100\n",
      "F:\\web3_messing_around\\PostThread-Polygon\\backend\\db_helpers.py:154: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['social_score'] = social_score\n",
      "F:\\web3_messing_around\\PostThread-Polygon\\backend\\db_helpers.py:155: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['level'] = level\n",
      "F:\\web3_messing_around\\PostThread-Polygon\\backend\\db_helpers.py:156: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['overall_score'] = overall_score\n",
      "F:\\web3_messing_around\\PostThread-Polygon\\backend\\db_helpers.py:157: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['overall_score'] /= max(overall_score)\n",
      "F:\\web3_messing_around\\PostThread-Polygon\\backend\\db_helpers.py:158: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['exp'] = exp\n",
      "F:\\web3_messing_around\\PostThread-Polygon\\backend\\db_helpers.py:159: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['exp_to_next_level'] = exp_to_next_level\n",
      "F:\\web3_messing_around\\PostThread-Polygon\\backend\\db_helpers.py:160: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['daily_payout_value'] = daily_token_rewards * df['overall_score']\n",
      "F:\\web3_messing_around\\PostThread-Polygon\\backend\\db_helpers.py:161: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[df['daily_payout_value'] < 100] = 100\n"
     ]
    }
   ],
   "source": [
    "# Daily payout\n",
    "for idx, row in all_data['users'][all_data['users']['daily_payout_claimed'] == True].iterrows():\n",
    "    updates = {\n",
    "        \"daily_payout_claimed\": False, \n",
    "        \"tokens_to_claim\": row['tokens_to_claim'] + row['daily_payout_value']\n",
    "    }\n",
    "    supabase.table(\"users\").update(updates).eq(\"id\", row['id']).execute()\n",
    "    \n",
    "all_users = all_data['users'].copy()\n",
    "all_users.loc[all_users['username'].isna(), :] = update_scores(all_data, all_users[all_users['username'].isna()])\n",
    "all_users.loc[~all_users['username'].isna(), :] = update_scores(all_data, all_users[~all_users['username'].isna()])\n",
    "\n",
    "for idx, row in all_users.iterrows():\n",
    "    updates = {\n",
    "        'social_score': row['social_score'],\n",
    "        'level': row['level'],\n",
    "        'exp': row['exp'],\n",
    "        'exp_to_next_level': row['exp_to_next_level'],\n",
    "        'daily_payout_value': row['daily_payout_value'],\n",
    "    }\n",
    "    supabase.table(\"users\").update(updates).eq(\"id\", row['id']).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91aa867",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reddit Airdrop\n",
    "\n",
    "# Update airdrop value\n",
    "for idx, row in all_data['users'][~all_data['users']['reddit_username'].isna() & all_data['users']['reddit_airdrop_value'].isna()  & ~all_data['users']['username'].isna()].iterrows():\n",
    "    user = reddit.redditor(row['reddit_username'])   \n",
    "    try:\n",
    "        airdrop_value = user.total_karma\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    supabase.table(\"users\").update({\"reddit_airdrop_value\": airdrop_value}).eq(\"id\", row['id']).execute()\n",
    "print(\"Done update airdrop value\")\n",
    "                \n",
    "# Check for claims\n",
    "for idx, row in all_data['users'][all_data['users']['reddit_airdrop_claimed'] == \"pending\"].iterrows():\n",
    "    if row['reddit_id'] is None:\n",
    "        supabase.table(\"users\").update({\"reddit_airdrop_claimed\": \"rejected\"}).eq(\"id\", row['id']).execute()\n",
    "    \n",
    "    user = reddit.redditor(reddit_username)  \n",
    "    try:\n",
    "        for i, post in enumerate(user.new()):\n",
    "            if i > 10:\n",
    "                break\n",
    "            if type(post) == praw.models.reddit.submission.Submission:\n",
    "                if row[\"wallet_address_personal\"] in post.selftext:\n",
    "                    airdrop_value = user.total_karma\n",
    "                    updates = {\n",
    "                        \"reddit_airdrop_value\": airdrop_value,\n",
    "                        \"reddit_airdrop_claimed\": \"claimed\",\n",
    "                        \"tokens_to_claim\": row['tokens_to_claim'] + airdrop_value,\n",
    "                    }\n",
    "                    supabase.table(\"users\").update(updates).eq(\"id\", row['id']).execute()\n",
    "    except:\n",
    "        supabase.table(\"users\").update({\"reddit_airdrop_claimed\": \"rejected\"}).eq(\"id\", row['id']).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac14c2c",
   "metadata": {},
   "source": [
    "## Blockchain stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d57d4b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transfer tokens\n",
    "\n",
    "for idx, row in all_data['users'][(all_data['users']['tokens_claimed'] == True) & (all_data['users']['tokens_to_claim'] != 0) & (~all_data['users']['wallet_address_personal'].isna())].iterrows():\n",
    "    with open(\"new_announcements.txt\", \"a\") as file_object:\n",
    "        file_object.write(json.dumps({\"type\": \"mint_tokens\", \"wallet\": row[\"wallet_address_personal\"], \"amount\": row[\"tokens_to_claim\"]}) + '\\n')\n",
    "    supabase.table(\"users\").update({\"tokens_claimed\": False, \"tokens_to_claim\": 0}).eq(\"id\", row['id']).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "80e14633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Destote\n",
      "NachoStash\n",
      "redbrick5\n",
      "fussyboi27\n",
      "EbolaMan123\n",
      "Trevor-On-Reddit\n",
      "R-T-O-B\n",
      "No-Vacation-654\n",
      "yottabyte10008\n"
     ]
    }
   ],
   "source": [
    "# Mint users\n",
    "top_users = all_data['users'][~all_data['users']['username'].isna() & all_data['users']['msa_id'].isna()]\n",
    "top_users = top_users.sort_values(\"exp\")\n",
    "\n",
    "for idx, row in top_users.iterrows():\n",
    "    print(row['username'])\n",
    "    wallet = get_wallet_from_username(row['username'])\n",
    "    with open(\"new_announcements.txt\", \"a\") as file_object:\n",
    "        file_object.write(json.dumps({\"type\": \"new_user\", \"private_key\": wallet.privateKey.hex(), \"row_id\": row['id']}) + '\\n')\n",
    "    supabase.table(\"users\").update({\"wallet_address_provided\": wallet.address}).eq(\"id\", row['id']).execute()\n",
    "    \n",
    "# Mint users with msa\n",
    "top_users = all_data['users'][~all_data['users']['username'].isna() & ~all_data['users']['msa_id'].isna()]\n",
    "top_users = top_users.sort_values(\"exp\")\n",
    "\n",
    "for idx, row in top_users.iterrows():\n",
    "    wallet = get_wallet_from_username(row['username'])\n",
    "    message = mint_user(row['msa_id'], row['profile_pic'], wallet.address)\n",
    "    with open(\"new_announcements.txt\", \"a\") as file_object:\n",
    "        file_object.write(json.dumps({\"type\": \"mint_data\", \"message\": message, \"schemaId\": schemas[\"user\"], \"user_msa_id\": row[\"msa_id\"], \"row_id\": row['id']}) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b4172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mint posts\n",
    "top_posts = all_data['posts'][all_data['posts']['reddit_id'].isna() & all_data['posts']['transaction_hash'].isna() & ~all_data['posts']['ipfs_hash'].isna()]\n",
    "top_posts = top_posts.join(all_data['users'].set_index('id')[['username', 'msa_id', 'tokens_to_claim']], on=\"user_id\")\n",
    "top_posts = top_posts[~top_posts['msa_id'].isna() & ~top_posts['username'].isna() & top_posts['tokens_to_claim'] > 5]\n",
    "top_posts = top_posts.join(all_data['votes'].groupby(\"post_id\").sum()[['up']], on=\"id\", how=\"inner\")\n",
    "top_posts = top_posts.sort_values(\"up\")\n",
    "\n",
    "for idx, row in top_posts.iterrows():\n",
    "    message = json.dumps({\"ipfs_hash\": row['ipfs_hash']})\n",
    "    with open(\"new_announcements.txt\", \"a\") as file_object:\n",
    "        file_object.write(json.dumps({\"type\": \"mint_data\", \"message\": message, \"schemaId\": \"post\", \"user_msa_id\": row[\"msa_id\"], \"row_id\": row['id']}) + '\\n')\n",
    "    supabase.table(\"users\").update({\"tokens_claimed\": False, \"tokens_to_claim\": 0}).eq(\"id\", row['id']).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc48b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c44244e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
